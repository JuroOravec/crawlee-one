/**
 * Central configuration for the benchmarking infrastructure.
 *
 * All paths, thresholds, and settings used by the benchmark files,
 * the transform script, and the CI workflow are defined here.
 *
 * The GitHub Actions workflow (.github/workflows/crawlee-one--benchmark.yml) reads
 * these values at runtime via `benchmarks/run-bench.ts export`, which
 * writes them to $GITHUB_ENV as BENCH_* variables. This file is the
 * single source of truth -- no manual sync needed.
 */

import path from 'node:path';
import type { BenchConfig } from './run-bench.js';

// ---------------------------------------------------------------------------
// Resolve helpers -- all paths are relative to the project root
// ---------------------------------------------------------------------------

const benchDir = import.meta.dirname ?? __dirname;
const packageDir = path.resolve(benchDir, '..');
const gitRootDir = path.resolve(packageDir, '../..');

const fromPackage = (...segments: string[]) => path.resolve(packageDir, ...segments);

// ---------------------------------------------------------------------------
// Config
// ---------------------------------------------------------------------------

export const benchConfig = {
  /** Schema version for the per-commit result JSON files in .benchmarks/results/. */
  resultVersion: 1,

  // -- File paths -----------------------------------------------------------

  /** Absolute path to the project's git root (monorepo workspace root). */
  gitRootDir,

  /** Path to the project's package.json (used to read project metadata). */
  packageJsonPath: fromPackage('package.json'),

  /** Vitest bench JSON output (generated by `pnpm run bench:gen`). */
  vitestOutputFile: fromPackage('benchmarks', 'temp', 'results.json'),

  /** Sidecar file storing test results metadata (written by helpers.ts, consumed by run-bench.ts generate). */
  sidecarFile: fromPackage('benchmarks', 'temp', 'sidecar.json'),

  /** Dashboard JSON for github-action-benchmark (written by run-bench.ts generate). */
  dashboardOutputFile: fromPackage('benchmarks', 'temp', 'dashboard.json'),

  /** Root directory for persisted benchmark results (committed to the repo). */
  resultsDir: fromPackage('benchmarks', 'data'),

  // -- Tracked dependencies -------------------------------------------------

  /**
   * Package names whose installed versions are recorded in each benchmark
   * result file. Add entries here when new dependencies meaningfully affect
   * benchmark performance.
   */
  trackedDependencies: ['crawlee', 'vitest', 'playwright', 'cheerio'],

  // -- github-action-benchmark settings -------------------------------------
  //
  // Exported to $GITHUB_ENV by `run-bench.ts export` and consumed
  // by the workflow steps via ${{ env.BENCH_* }} references.

  /** Display name shown in the dashboard chart header. */
  name: 'Benchmark',

  /** Branch that hosts the GitHub Pages dashboard. */
  ghPagesBranch: 'gh-pages',

  /** Directory path on the gh-pages branch where dashboard data lives. */
  benchmarkDataDirPath: 'dev/bench',

  /**
   * Percentage threshold for flagging a regression in PR comments.
   * "115%" means a 15% regression triggers an alert.
   */
  alertThreshold: '110%',

  /** Whether the CI workflow should fail when a regression exceeds alertThreshold. */
  failOnAlert: true,

  /** Post a comparison comment on every PR, not only when an alert fires. */
  commentAlways: true,
} satisfies BenchConfig;
